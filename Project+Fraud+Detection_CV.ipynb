{"cells":[{"cell_type":"markdown","source":["# Fraud Detection"],"metadata":{"collapsed":true}},{"cell_type":"markdown","source":["## Import Spark SQL and Spark ML Libraries"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.classification import LogisticRegression"],"metadata":{"collapsed":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Load Source Data"],"metadata":{}},{"cell_type":"markdown","source":["The data for the project is about mobile money transactions based on a sample of real transactions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, who is the provider of the mobile financial service which is currently running in more than 14 countries all around the world.\n\nThis synthetic dataset is scaled down 1/4 of the original dataset and it is created just for Kaggle.\nThe data comes in a .csv format."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\n# @hidden_cell\n# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef set_hadoop_config_with_credentials_f63cbb38899d47179c49ed4a7cf03ccf(name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', '265dd6d8a99a4549a24ac9574846808d')\n    hconf.set(prefix + '.username', '886a93bbc2564a539f02a62ed61e1a61')\n    hconf.set(prefix + '.password', 'h8bjj]J[1DME7LnC')\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)\n\n# you can choose any name\nname = 'keystone'\nset_hadoop_config_with_credentials_f63cbb38899d47179c49ed4a7cf03ccf(name)\n\nspark = SparkSession.builder.getOrCreate()\n\n#df = spark.read\\\n  #.format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  #.option('header', 'true')\\\n  #.option(\"inferSchema\", \"true\")\\\n  #.load('swift://ITBSProjectFraudDetection.' + name + '/frauddetectionsmall.csv')\ndf = sqlContext.sql(\"SELECT * FROM frauddetection\")\ndf.take(5)\ndf.dtypes\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":6},{"cell_type":"code","source":["\n'''df = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .option(\"inferSchema\", \"true\")\\\n  .load('swift://ITBSProjectFraudDetection.' + name + '/frauddetectionsmall.csv')\ndf.take(5)'''\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":7},{"cell_type":"code","source":["'''%matplotlib inline\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfraud = df.toPandas()\nf, ax = plt.subplots(1, 1, figsize=(8, 8))\nfraud.type.value_counts().plot(kind='bar', title=\"Transaction type\", ax=ax, figsize=(8,8))\nplt.show()\nplt.figure(0)\ncond = (fraud['isFraud'] >= 1)\ntaf = fraud[cond].type.value_counts().plot(kind='bar',  title=\"Fraud transactions grouped by type\")\nplt.show(taf)\nplt.figure(1)\ncond2 = (fraud['isFraud'] < 1)\ntaf2 = fraud[cond2].type.value_counts().plot(kind='bar',  title=\"No fraud transactions grouped by type\")\nplt.show(taf2)\n#fraud['type'] = fraud['type'].apply(convert)\n#fraud1 = normalize(fraud)\nplt.figure(2)\nmedianprops = dict(linestyle='-', linewidth=2, color='blue')\nbx1 = fraud[cond2].boxplot(column=['oldbalanceDest', 'newbalanceDest'], by='isFraud', medianprops=medianprops)\nbx2 = fraud[cond2].boxplot(column=['oldbalanceOrg', 'newbalanceOrig'], by='isFraud', medianprops=medianprops)\nbx3 = fraud[cond2].boxplot(column=['amount'], by='isFraud', medianprops=medianprops)\n#bx4 = fraud[cond].boxplot(column=['type'], by='isFraud', medianprops=medianprops)\nplt.show(bx1)\nplt.show(bx2)\nplt.show(bx3)\n#plt.figure(3)\n#hist1 = fraud[cond2].plot.hist(by='isFraud', stacked=True, bins=20)'''"],"metadata":{"collapsed":false},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## Scatter Plots\nNext, we want to interpret trends in our data using scatter plots."],"metadata":{}},{"cell_type":"code","source":["'''plt.figure(4)\nsc1 = fraud.plot.scatter(x='oldbalanceDest', y='newbalanceDest')\nsc2 = fraud.plot.scatter(x='oldbalanceOrg', y='newbalanceOrig')\nsc1 = fraud.plot.scatter(x='oldbalanceDest', y='oldbalanceOrg')\nsc2 = fraud.plot.scatter(x='oldbalanceOrg', y='newbalanceDest')\nsc3 = fraud.plot.scatter(x='amount', y='isFraud')\nsc4 = fraud.plot.scatter(x='oldbalanceDest', y='isFraud')\nsc5 = fraud.plot.scatter(x='newbalanceDest', y='isFraud')\nsc6 = fraud.plot.scatter(x='oldbalanceOrg', y='isFraud')\nsc7 = fraud.plot.scatter(x='newbalanceOrig', y='isFraud')\nplt.show(sc1)\nplt.show(sc2)\nplt.show(sc3)\nplt.show(sc4)\nplt.show(sc5)\nplt.show(sc6)\nplt.show(sc7)'''"],"metadata":{"collapsed":false},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## Clear the data\nIn the next step, we will drop the columns that are useless for our model."],"metadata":{}},{"cell_type":"code","source":["df2 = df.select(\"type\", \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\", (col(\"isFraud\").cast(\"Int\").alias(\"label\")))\ndf2.take(5)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["## Split the data\nIn the next step we split the data in a train and test set."],"metadata":{}},{"cell_type":"code","source":["splits = df2.randomSplit([0.7, 0.3])\ntrain = splits[0]\ntest = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\ntrain_rows = train.count()\ntest_rows = test.count()\nprint \"Training Rows:\", train_rows, \" Testing Rows:\", test_rows\ntrain.show(5)\ntest.show(5)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## Define a pipeline and train the model\nWe need to prepare the features."],"metadata":{}},{"cell_type":"code","source":["strIdx = StringIndexer(inputCol = \"type\", outputCol = \"typeCat\")\nlabelIdx = StringIndexer(inputCol = \"label\", outputCol = \"idxLabel\")\n# number is meaningful so that it should be number features\ncatVect = VectorAssembler(inputCols = [\"typeCat\"], outputCol=\"catFeatures\")\ncatIdx = VectorIndexer(inputCol = catVect.getOutputCol(), outputCol = \"idxCatFeatures\")\nnumVect = VectorAssembler(inputCols = [\"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\"], outputCol=\"numFeatures\")\n# number vector is normalized\nminMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normFeatures\")\nfeatVect = VectorAssembler(inputCols=[\"idxCatFeatures\", \"normFeatures\"], outputCol=\"features\")\n\nrf = RandomForestClassifier(labelCol=\"idxLabel\", featuresCol=\"features\")\npi = Pipeline(stages=[strIdx, labelIdx, catVect, catIdx, numVect, minMax, featVect, rf])\n\n# Pipeline process the series of transformation above, which is 7 transformation\nprint('Pipeline completed')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#paramGrid2 = (ParamGridBuilder().addGrid(dt.impurity, (\"gini\", \"entropy\")).addGrid(rf.maxDepth, [5, 10, 20]).addGrid(rf.maxBins, [5, 10, 20]).build())\n\nparamGrid = (ParamGridBuilder().addGrid(rf.impurity, (\"gini\", \"entropy\")).addGrid(rf.maxDepth, [5, 10, 20]).addGrid(rf.maxBins, [5, 10, 20]).build())\ncv = CrossValidator(estimator=pi, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid, numFolds=3)\n#cv2 = CrossValidator(estimator=pipeline[1], evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid2, numFolds=5)\n#cv2 = TrainValidationSplit(estimator=pipeline[1], evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid2, trainRatio=0.8)\n#cv2 = TrainValidationSplit(estimator=pipeline[1], evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid2, trainRatio=0.8)\nmodel = cv.fit(train)\nprint \"Model completed\"\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## Test the model\nWe transform the test dataframe to generate label predictions."],"metadata":{}},{"cell_type":"code","source":["'''predictions = model.transform(test)\npredicted = predictions.select(\"features\", \"prediction\", \"probability\", \"trueLabel\")\npredicted.show(100, truncate=False)\nfor row in predicted.collect():\n    print row'''\n\nprediction = model.transform(test)\npredicted = prediction.select(\"features\", \"prediction\", \"probability\", \"trueLabel\")\npredicted.show(30)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["## Evaluation\nIn the next step we evaluate the model"],"metadata":{}},{"cell_type":"code","source":["#from pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nevaluator = BinaryClassificationEvaluator(\n    labelCol=\"trueLabel\", rawPredictionCol=\"prediction\")\n#evaluator = MulticlassClassificationEvaluator(\n#labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nareUPR = evaluator.evaluate(predicted, {evaluator.metricName: \"areaUnderPR\"})\nareUROC = evaluator.evaluate(predicted, {evaluator.metricName: \"areaUnderROC\"})\nprint(\"AreaUnderPR = %g \" % (areUPR))\nprint(\"AreaUnderPR = %g \" % (areUROC))\n\ntp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 1\").count())\nfp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 0\").count())\ntn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 0\").count())\nfn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 1\").count())\n\nprecision = tp / (tp + fp)\nrecall = tp / (tp + fn)\nprint(\"Precision = %g \" % (precision))\nprint(\"Recall = %g \" % (recall))\n\nmetrics = sqlContext.createDataFrame([\n(\"TP\", tp),\n(\"FP\", fp),\n(\"TN\", tn),\n(\"FN\", fn),\n(\"Precision\", tp / (tp + fp)),\n(\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\nmetrics.show()\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":21}],"metadata":{"celltoolbar":"Raw Cell Format","kernelspec":{"display_name":"Python 2 with Spark 2.1","language":"python","name":"python2-spark21"},"name":"Project+Fraud+Detection 1 model","notebookId":610820515794471},"nbformat":4,"nbformat_minor":0}
