{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.calstatela.edu/centers/hipic\"><img align=\"left\" src=\"https://avatars2.githubusercontent.com/u/4156894?v=3&s=100\"><image/>\n",
    "</a>\n",
    "<img align=\"right\" alt=\"California State University, Los Angeles\" src=\"http://www.calstatela.edu/sites/default/files/groups/California%20State%20University%2C%20Los%20Angeles/master_logo_full_color_horizontal_centered.svg\" style=\"width: 360px;\"/>\n",
    "\n",
    "#    CIS5560 Term Project Tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "#### Authors: Bhagyashree Bhagwat, Niklas Meher, Priyanka Purushu\n",
    "\n",
    "#### Instructor: [Jongwook Woo](https://www.linkedin.com/in/jongwook-woo-7081a85)\n",
    "\n",
    "#### Date: 05/18/2017\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Fraud Transaction Detection using Spark ML\n",
    "\n",
    "## Abstract:\n",
    "This project aims at analyzing data and providing insights on Financial Fraud Detection using Spark ML. The dataset contains a sample of real transactions extracted from one month of financial logs from a mobile money service.The original logs were provided by the multinational company, who is the provider of the mobile financial service which is currently running in more than 14 countries all around the world. This dataset is scaled down to 1/4 of the original dataset and it is created just for Kaggle. \n",
    "\n",
    "## Dataset specification\n",
    "- File format : *.csv* format\n",
    "- url : *(https://www.kaggle.com/ntnu-testimon/paysim1)*\n",
    "\n",
    "We took the whole dataset and tried three different classification models in spark Ml and performed data preparation, model building and validation and finally model evaluation and interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Spark SQL and Spark ML Libraries\n",
    "Import all the Spark SQL and ML libraries as mentioned below. This is neccessary to access the functions available in those libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Source Data\n",
    "\n",
    "In order to upload the .csv file i.e. our dataset follow the below mentioned steps:-\n",
    "\n",
    "1. Go to tables and click on create table. In the data source choose *file* if it is not in the *DBFS (Databricks File System)* and import the .csv file from the local system.\n",
    "2. The .csv file will be stored in an unique location in your DBFS and the path where the file is stored will appear against *uploaded to DBFS*. Note down the file path as it is mandatory to provide this path when we call spark.read function.\n",
    "3. Click on the *peview table* option and provide a table name, choose the file type e.g. CSV and the choose the delimiter based on your file.\n",
    "4. Once the file is uploaded to DBFS now call the function *spark.read.format().option().load()* and provide the file path is the load function. Assign this read function to a variable as mentioned in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "def set_hadoop_config_with_credentials_f63cbb38899d47179c49ed4a7cf03ccf(name):\n",
    "    \"\"\"This function sets the Hadoop configuration so it is possible to\n",
    "    access data from Bluemix Object Storage using Spark\"\"\"\n",
    "\n",
    "    prefix = 'fs.swift.service.' + name\n",
    "    hconf = sc._jsc.hadoopConfiguration()\n",
    "    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n",
    "    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n",
    "    hconf.set(prefix + '.tenant', '265dd6d8a99a4549a24ac9574846808d')\n",
    "    \n",
    "    \n",
    "    hconf.set(prefix + '.username', '886a93bbc2564a539f02a62ed61e1a61')\n",
    "    hconf.set(prefix + '.password', 'h8bjj]J[1DME7LnC')\n",
    "    hconf.setInt(prefix + '.http.port', 8080)\n",
    "    hconf.set(prefix + '.region', 'dallas')\n",
    "    hconf.setBoolean(prefix + '.public', False)\n",
    "\n",
    "# you can choose any name\n",
    "name = 'keystone'\n",
    "set_hadoop_config_with_credentials_f63cbb38899d47179c49ed4a7cf03ccf(name)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# change the file path in load with your own file path\n",
    "df = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load('swift://ITBSProjectFraudDetection.' + name + '/frauddetectionsmall.csv')\n",
    "df.take(5)\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "Visualization of the data to get a better understanding of it. In this project we have done bar plot and scatter plot visualisation of the data.\n",
    "\n",
    "**1. Insights from Bar plot**\n",
    "\n",
    "   - Amount of transaction grouped by type\n",
    "    - We see that most of the fraud transactions are made with *CASH_OUT* and *PAYMENTS*\n",
    "   - Fraud transaction grouped by *CASH_OUT* and *PAYMENT*\n",
    "    - Fraud transactions are only made with the type *CASH_OUT* and *TRANSFER*. This is an interesting fact since the type TRANSFER is in the fourth place when it comes to the number of transactions. \n",
    "    \n",
    "**2. Insights from Scatter plot**\n",
    "\n",
    " - Since the points on scatter plots are alligned closely in a linear manner we can determine that there is a direct relatioship between the attributes 'newbalanceDest' and 'oldbalanceDest' as well as between 'newbalanceOrig' and 'oldbalanceOrg'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fraud = df.toPandas()\n",
    "f, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "fraud.type.value_counts().plot(kind='bar', title=\"Transaction type\", ax=ax, figsize=(8,8))\n",
    "plt.show()\n",
    "plt.figure(0)\n",
    "cond = (fraud['isFraud'] >= 1)\n",
    "taf = fraud[cond].type.value_counts().plot(kind='bar',  title=\"Fraud transactions grouped by type\")\n",
    "plt.show(taf)\n",
    "plt.figure(1)\n",
    "cond2 = (fraud['isFraud'] < 1)\n",
    "taf2 = fraud[cond2].type.value_counts().plot(kind='bar',  title=\"No fraud transactions grouped by type\")\n",
    "plt.show(taf2)\n",
    "fraud.hist(column='isFlaggedFraud', bins=5)\n",
    "plt.show()\n",
    "plt.figure(2)\n",
    "medianprops = dict(linestyle='-', linewidth=2, color='blue')\n",
    "bx1 = fraud[cond2].boxplot(column=['oldbalanceDest', 'newbalanceDest'], by='isFraud', medianprops=medianprops)\n",
    "bx2 = fraud[cond2].boxplot(column=['oldbalanceOrg', 'newbalanceOrig'], by='isFraud', medianprops=medianprops)\n",
    "bx3 = fraud[cond2].boxplot(column=['amount'], by='isFraud', medianprops=medianprops)\n",
    "#bx4 = fraud[cond].boxplot(column=['type'], by='isFraud', medianprops=medianprops)\n",
    "plt.show(bx1)\n",
    "plt.show(bx2)\n",
    "plt.show(bx3)\n",
    "plt.figure(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fraud.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plots\n",
    "\n",
    "Next, we want to find correlations between attributes using scatter plots. It's visible that there are correlations between the attributes 'newbalanceDest' and 'oldbalanceDest' as well as between 'newbalanceOrig' and 'oldbalanceOrg'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(4)\n",
    "sc1 = fraud.plot.scatter(x='oldbalanceDest', y='newbalanceDest')\n",
    "sc2 = fraud.plot.scatter(x='oldbalanceOrg', y='newbalanceOrig')\n",
    "sc1 = fraud.plot.scatter(x='oldbalanceDest', y='oldbalanceOrg')\n",
    "sc2 = fraud.plot.scatter(x='oldbalanceOrg', y='newbalanceDest')\n",
    "sc3 = fraud.plot.scatter(x='amount', y='isFraud')\n",
    "sc4 = fraud.plot.scatter(x='oldbalanceDest', y='isFraud')\n",
    "sc5 = fraud.plot.scatter(x='newbalanceDest', y='isFraud')\n",
    "sc6 = fraud.plot.scatter(x='oldbalanceOrg', y='isFraud')\n",
    "sc7 = fraud.plot.scatter(x='newbalanceOrig', y='isFraud')\n",
    "plt.show(sc1)\n",
    "plt.show(sc2)\n",
    "plt.show(sc3)\n",
    "plt.show(sc4)\n",
    "plt.show(sc5)\n",
    "plt.show(sc6)\n",
    "plt.show(sc7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear the data\n",
    "\n",
    "In the next step, we will select the columns that are useful for our model. This is done in order to avoid the ppresence of outliners in the model. \n",
    "\n",
    "1. The columns that we have removed are:\n",
    "  -   step\n",
    "  -  nameDest\n",
    "  -  nameOrig\n",
    "  -  IsFlaggedFraud\n",
    "\n",
    "2. Columns retained are:\n",
    "  -  amount\n",
    "  -  oldbalanceOrg\n",
    "  -  newbalanceOrig\n",
    "  -  oldbalanceDest\n",
    "  -  newbalanceDest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = df.select(\"type\", \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\", (col(\"isFraud\").cast(\"Int\").alias(\"label\")))\n",
    "df2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data\n",
    "In the next step we split the data in a train and test set. We have split the data in the ratio of **70 to 30**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splits = df2.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "test = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\n",
    "train_rows = train.count()\n",
    "test_rows = test.count()\n",
    "print \"Training Rows:\", train_rows, \" Testing Rows:\", test_rows\n",
    "train.show(5)\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a pipeline\n",
    "Pipeline provides a simple construction, tuning and testing for ML workflows.\n",
    "\n",
    "## Algorithms used to Train the model\n",
    "\n",
    "In this project we have used the following algorithm to train our model:\n",
    "\n",
    "1. Random Forest Classifier (RF)\n",
    "2. Decision Tree Classifier (DT) \n",
    "3. Logistic Regression (LR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strIdx = StringIndexer(inputCol = \"type\", outputCol = \"typeCat\")\n",
    "labelIdx = StringIndexer(inputCol = \"label\", outputCol = \"idxLabel\")\n",
    "# number is meaningful so that it should be number features\n",
    "catVect = VectorAssembler(inputCols = [\"typeCat\"], outputCol=\"catFeatures\")\n",
    "catIdx = VectorIndexer(inputCol = catVect.getOutputCol(), outputCol = \"idxCatFeatures\")\n",
    "numVect = VectorAssembler(inputCols = [\"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\"], outputCol=\"numFeatures\")\n",
    "# number vector is normalized\n",
    "minMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normFeatures\")\n",
    "featVect = VectorAssembler(inputCols=[\"idxCatFeatures\", \"normFeatures\"], outputCol=\"features\")\n",
    "\n",
    "cl = []\n",
    "pipeline = []\n",
    "\n",
    "\n",
    "cl.insert(0, DecisionTreeClassifier(labelCol=\"idxLabel\", featuresCol=\"features\"))\n",
    "cl.insert(1, RandomForestClassifier(labelCol=\"idxLabel\", featuresCol=\"features\"))\n",
    "cl.insert(2, LogisticRegression(labelCol=\"idxLabel\", featuresCol=\"features\"))\n",
    "\n",
    "\n",
    "# Pipeline process the series of transformation above, which is 7 transformation\n",
    "for i in range(3):\n",
    "    pipeline.insert(i, Pipeline(stages=[strIdx, labelIdx, catVect, catIdx, numVect, minMax, featVect, cl[i]]))\n",
    "    #piplineModel = pipeline.fit(train)\n",
    "print \"Pipeline complete!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validation Split\n",
    "\n",
    "we used a train validation split instead of cross validation for every model because it takes much less time to train the model with the train validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = []\n",
    "\n",
    "#When using the whole dataset, please use the TrainValidationSplit instead of the CrossValidator!\n",
    "\n",
    "paramGrid = (ParamGridBuilder().addGrid(cl[0].impurity, (\"gini\", \"entropy\")).addGrid(cl[0].maxDepth, [5, 10, 20]).addGrid(cl[0].maxBins, [5, 10, 20]).build())\n",
    "cv = CrossValidator(estimator=pipeline[0], evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid, numFolds=5)\n",
    "#cv = TrainValidationSplit(estimator=pipeline[0], evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid, trainRatio=0.8)\n",
    "model.insert(0, cv.fit(train))\n",
    "print \"Model 1 completed\"\n",
    "\n",
    "\n",
    "paramGrid2 = (ParamGridBuilder().addGrid(cl[1].impurity, (\"gini\", \"entropy\")).addGrid(cl[1].maxDepth, [5, 10, 20]).addGrid(cl[1].maxBins, [5, 10, 20]).build())\n",
    "cv2 = CrossValidator(estimator=pipeline[1], evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid2, numFolds=5)\n",
    "#cv2 = TrainValidationSplit(estimator=pipeline[1], evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid2, trainRatio=0.8)\n",
    "model.insert(1, cv2.fit(train))\n",
    "print \"Model 2 completed\"\n",
    "\n",
    "paramGrid3 = (ParamGridBuilder().addGrid(cl[2].regParam, [0.01, 0.5, 2.0]).addGrid(cl[2].threshold, [0.30, 0.35, 0.5]).addGrid(cl[2].maxIter, [1, 5]).addGrid(cl[2].elasticNetParam, [0.0, 0.5, 1]).build())\n",
    "cv3 = CrossValidator(estimator=pipeline[2], evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid3, numFolds=5)\n",
    "#cv3 = TrainValidationSplit(estimator=pipeline[2], evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid, trainRatio=0.8)\n",
    "model.insert(2, cv3.fit(train))\n",
    "print \"Model 3 completed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "We transform the test dataframe to generate label predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''predictions = model.transform(test)\n",
    "predicted = predictions.select(\"features\", \"prediction\", \"probability\", \"trueLabel\")\n",
    "predicted.show(100, truncate=False)\n",
    "for row in predicted.collect():\n",
    "    print row'''\n",
    "prediction = [] \n",
    "predicted = []\n",
    "for i in range(3):\n",
    "  prediction.insert(i, model[i].transform(test))\n",
    "  predicted.insert(i, prediction[i].select(\"features\", \"prediction\", \"probability\", \"trueLabel\"))\n",
    "  predicted[i].show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "In this step we evaluate the model. The important measuring metrics that we have based our evaluation on are **Recall, Precision and AUC**. The values of these metrics helps in indicating how good we are in detecting fraud transactions when the transaction is actually a fraud.\n",
    "\n",
    "Recall and Precision are calculated based on the TP,FP and FN values from the confusion matrix. The formulaes to calculate Recall and precision are given below.\n",
    "1. **Recall    = TP / (TP + FN)**\n",
    "2. **Precision = TP / (TP + FP)**\n",
    "\n",
    "Description of the Terms used above are:\n",
    "-  TP : True positives \n",
    "-  FP : False Positives i.e. values that are not positive\n",
    "-  FN : False Negative i.e. values that are positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"trueLabel\", rawPredictionCol=\"prediction\")\n",
    "for i in range(3):\n",
    "    #evaluator = MulticlassClassificationEvaluator(\n",
    "    #labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "    areUPR = evaluator.evaluate(predicted[i], {evaluator.metricName: \"areaUnderPR\"})\n",
    "    areUROC = evaluator.evaluate(predicted[i], {evaluator.metricName: \"areaUnderROC\"})\n",
    "    print(\"AreaUnderPR = %g \" % (areUPR))\n",
    "    \n",
    "    print(\"AreaUnderROC = %g \" % (areUROC))\n",
    "\n",
    "    tp = float(predicted[i].filter(\"prediction == 1.0 AND truelabel == 1\").count())\n",
    "    fp = float(predicted[i].filter(\"prediction == 1.0 AND truelabel == 0\").count())\n",
    "    tn = float(predicted[i].filter(\"prediction == 0.0 AND truelabel == 0\").count())\n",
    "    fn = float(predicted[i].filter(\"prediction == 0.0 AND truelabel == 1\").count())\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    print(\"Precision = %g \" % (precision))\n",
    "    print(\"Recall = %g \" % (recall))\n",
    "\n",
    "    metrics = sqlContext.createDataFrame([\n",
    "    (\"TP\", tp),\n",
    "    (\"FP\", fp),\n",
    "    (\"TN\", tn),\n",
    "    (\"FN\", fn),\n",
    "    (\"Precision\", tp / (tp + fp)),\n",
    "    (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\n",
    "    metrics.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "raw_mimetype": "text/html"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The metrics values for the 3 algorithms are as follows:\n",
    "\n",
    "| **Model**                   | **Area Under ROC**| **Precision**| **Recall **| \n",
    "|:----------------------- |:-------------:| :--------:|:-------:|\n",
    "| DecisionTreeClassifier  | 0.839343      | 0.965338 |0.678716|\n",
    "| RandomForestClassifier  | 0.85963       | 0.92674  |0.719334|\n",
    "| LogisticRegression      | 0.726389      | 0.845979 |0.452884|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RandomForestClassifier has the best recall score as compared to DecisionTreeClassifier and LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.1",
   "language": "python",
   "name": "python2-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "name": "Project+Fraud+Detection IBM",
  "notebookId": 721681162079156
 },
 "nbformat": 4,
 "nbformat_minor": 0
}